# -*- coding: utf-8 -*-
"""XGB + OP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WpPXHqC1htk590_JapnTAlG-tIb_y1S9
"""

!pip install scikit-optimize
!pip install requests
!pip install optuna

"""# Import Library"""

# Import libraries
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import requests
import xgboost as xgb
import optuna
import time
from sklearn.model_selection import train_test_split, TimeSeriesSplit, RandomizedSearchCV, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.feature_selection import SelectFromModel
from scipy.stats import uniform, randint
from skopt import BayesSearchCV
from skopt.space import Real, Integer

# Suppress warnings
warnings.filterwarnings('ignore')

# Mount Google Drive and load dataset
from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/My Drive/Dataset/dataset_merged.xlsx'
df = pd.read_excel(file_path)
print(f"Dataset loaded with {df.shape[0]} rows and {df.shape[1]} columns.")

"""# Data Cleaning

"""

# Standarisasi nama kolom
original_columns = df.columns.tolist()
df.columns = df.columns.str.strip().str.replace(" ", "_", regex=False)

# Check missing values
missing_values = df.isnull().sum()
missing_values = missing_values[missing_values > 0]

plt.figure(figsize=(10, 8))
plt.barh(range(len(df.columns)), [1]*len(df.columns))  # Membuat semua batang dengan tinggi 1
plt.yticks(range(len(df.columns)), df.columns)
plt.title('Daftar Fitur dalam Dataset')
plt.xlabel('Jumlah')
plt.ylabel('Fitur (Kolom)')
plt.tight_layout()
plt.show()

# Data Cleaning
df['Size'] = df['Size'].fillna('All_Size').astype(str).str.replace("Ld ", "", case=False).str.strip()
df['Payment_platform_discount'] = pd.to_numeric(df['Payment_platform_discount'], errors='coerce').fillna(0)
df['Handling_Fee'] = pd.to_numeric(df['Handling_Fee'], errors='coerce').fillna(0)

"""# Feature Engineering

Time Extraction
"""

original_columns = df.columns.tolist()

# Feature Engineering - Time Features
if 'Created_Time' in df.columns:
    df['Created_Time'] = pd.to_datetime(df['Created_Time'], errors='coerce', dayfirst=True)
    df.dropna(subset=['Created_Time'], inplace=True)

# Extract basic time features
df['year'] = df['Created_Time'].dt.year
df['month'] = df['Created_Time'].dt.month
df['day'] = df['Created_Time'].dt.day
df['hour'] = df['Created_Time'].dt.hour
df['minute'] = df['Created_Time'].dt.minute
df['second'] = df['Created_Time'].dt.second
df['day_of_week'] = df['Created_Time'].dt.dayofweek
df['day_of_year'] = df['Created_Time'].dt.dayofyear
df['quarter'] = df['Created_Time'].dt.quarter
df['week_of_year'] = df['Created_Time'].dt.isocalendar().week

# Cyclical transformations
df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
df['day_of_year_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)
df['day_of_year_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)
df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)

# Calendar/event features
df['is_payday'] = df['day'].isin([25, 26, 27, 28, 29, 30, 31, 1, 2]).astype(int)
df['date'] = df['Created_Time'].dt.date
df['year_month'] = df['Created_Time'].dt.strftime('%Y-%m')
df['year_week'] = df['year'].astype(str) + '-' + df['week_of_year'].astype(str).str.zfill(2)

# Flash sale features
flash_dates = [f"{str(i).zfill(2)}-{str(i).zfill(2)}" for i in range(1, 13)]
df['flash_date_str'] = df['month'].astype(str).str.zfill(2) + '-' + df['day'].astype(str).str.zfill(2)
df['is_flash_sale'] = df['flash_date_str'].isin(flash_dates).astype(int)

# National holidays
years = [2022, 2023, 2024, 2025]
holiday_dates = []

for year in years:
    url = f"https://date.nager.at/api/v3/PublicHolidays/{year}/ID"
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        for holiday in data:
            holiday_dates.append(holiday['date'])

libur_nasional = pd.to_datetime(holiday_dates)
df['is_holiday'] = df['Created_Time'].dt.date.isin(libur_nasional.date).astype(int)

# Additional flags
df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)
df['is_business_hour'] = (((df['hour'] >= 8) & (df['hour'] <= 17)) & (df['day_of_week'] < 5)).astype(int)
df['is_month_start'] = (df['day'] <= 7).astype(int)
df['is_month_end'] = (df['day'] >= 24).astype(int)

after_time_columns = df.columns.tolist()
added_time_features = [col for col in after_time_columns if col not in original_columns]

# Visualisasi
if added_time_features:
    fig, ax = plt.subplots(figsize=(8, len(added_time_features) * 0.3))
    bars = ax.barh(added_time_features, [1]*len(added_time_features), edgecolor='black')

    for bar in bars:
        ax.text(1.05, bar.get_y() + bar.get_height()/2, '1', va='center', fontsize=10)

    ax.set_title('Fitur Baru dari Feature Engineering Waktu', fontsize=14, pad=10)
    ax.set_xlabel('Fitur Baru')
    ax.set_xlim(0, 1.2)
    ax.set_xticks([])
    ax.invert_yaxis()
    plt.subplots_adjust(left=0.25, right=0.95, top=0.95, bottom=0.05)
    plt.grid(False)
    plt.show()

"""Time - Series"""

# Feature Engineering - Time Series Features
df = df.sort_values('Created_Time')
df['date'] = df['Created_Time'].dt.date

if 'Product_Name' in df.columns and 'Created_Time' in df.columns:
    # Daily sales aggregation per product
    daily_sales = df.groupby(['Product_Name', 'date'])['Quantity'].sum().reset_index()
    daily_sales = daily_sales.sort_values(['Product_Name', 'date'])

    # Create lag features
    for lag in [1, 7, 14, 30]:
        daily_sales[f'lag_{lag}_days'] = daily_sales.groupby('Product_Name')['Quantity'].shift(lag)

    # Moving Averages & Rolling Sum
    for window in [7, 14, 30]:
        daily_sales[f'moving_avg_{window}'] = daily_sales.groupby('Product_Name')['Quantity'].transform(
            lambda x: x.rolling(window=window, min_periods=1).mean()
        )
        daily_sales[f'rolling_sum_{window}'] = daily_sales.groupby('Product_Name')['Quantity'].transform(
            lambda x: x.rolling(window=window, min_periods=1).sum()
        )

    # Merge time series features to main dataframe
    df = pd.merge(
        df,
        daily_sales[['Product_Name', 'date'] +
                    [f'lag_{lag}_days' for lag in [1, 7, 14, 30]] +
                    [f'moving_avg_{window}' for window in [7, 14, 30]] +
                    [f'rolling_sum_{window}' for window in [7, 14, 30]]],
        on=['Product_Name', 'date'],
        how='left'
    )

    # Volatility features
    df['demand_std_7days'] = daily_sales.groupby('Product_Name')['Quantity'].transform(
        lambda x: x.rolling(window=7, min_periods=1).std()
    )

    # Sales trends and ratios
    df['sales_trend'] = (df['lag_7_days'] - df['lag_14_days']) / (df['lag_14_days'] + 1) * 100
    df['sales_ratio_to_avg'] = df['Quantity'] / (df['moving_avg_30'] + 1)

# Flash sale features
df['flash_sale_yesterday'] = df.groupby('Product_Name')['is_flash_sale'].shift(1).fillna(0)
df['days_since_last_flash'] = (
    df[::-1].groupby('Product_Name')['is_flash_sale']
    .apply(lambda x: x.cumsum().shift(-1).fillna(0))
    .reset_index(level=0, drop=True)[::-1]
)

# Holiday and payday features
df['holiday_yesterday'] = df['is_holiday'].shift(1).fillna(0)
df['payday_yesterday'] = df['is_payday'].shift(1).fillna(0)

# Additional time features
df['week_of_month'] = pd.to_datetime(df['date']).dt.day.apply(lambda d: (d - 1) // 7 + 1)

# Monthly seasonal index
if 'month' in df.columns:
    # Calculate monthly average per product
    monthly_avg = df.groupby(['Product_Name', 'month'])['Quantity'].mean().reset_index()

    # Calculate overall average per product
    product_avg = monthly_avg.groupby('Product_Name')['Quantity'].mean().reset_index()

    # Join to calculate seasonal index
    monthly_avg = pd.merge(monthly_avg, product_avg, on='Product_Name', suffixes=('_month', '_product'))
    monthly_avg['seasonal_index'] = monthly_avg['Quantity_month'] / monthly_avg['Quantity_product']

    # Join seasonal index to main dataframe
    df = pd.merge(df, monthly_avg[['Product_Name', 'month', 'seasonal_index']],
                  on=['Product_Name', 'month'], how='left')

# Fill NaN values for all time series features
ts_columns = [col for col in df.columns if 'lag_' in col or 'moving_avg_' in col or
              'trend' in col or 'ratio' in col or 'seasonal' in col or
              'std' in col or 'flash' in col or 'holiday_yesterday' in col or
              'payday_yesterday' in col]
for col in ts_columns:
    if col in df.columns:
        df[col] = df[col].fillna(0)

# Drop and Save
created_times = df['Created_Time'].copy()
product_names = df['Product_Name'].copy()
variations = df['Variation'].copy() if 'Variation' in df.columns else None
sizes = df['Size'].copy() if 'Size' in df.columns else None

# Drop non-feature columns
df.drop(columns=['Created_Time', 'date', 'year_month', 'year_week'], inplace=True, errors='ignore')
df.drop(columns=['Product_Name', 'Variation', 'Size'], inplace=True, errors='ignore')

after_ts_columns = df.columns.tolist()
added_ts_features = [col for col in after_ts_columns if col not in after_time_columns]

# Visualisasi
if added_ts_features:
    fig, ax = plt.subplots(figsize=(8, len(added_ts_features) * 0.3))
    bars = ax.barh(added_ts_features, [1]*len(added_ts_features), edgecolor='black')

    for bar in bars:
        ax.text(1.05, bar.get_y() + bar.get_height()/2, '1', va='center', fontsize=10)

    ax.set_title('Fitur Baru dari Feature Engineering Time Series', fontsize=14, pad=10)
    ax.set_xlabel('Fitur Baru')
    ax.set_xlim(0, 1.2)
    ax.set_xticks([])
    ax.invert_yaxis()
    plt.subplots_adjust(left=0.25, right=0.95, top=0.95, bottom=0.05)
    plt.grid(False)
    plt.show()

"""# Data Split"""

# Train-Validation-Test Split
target_column = 'Quantity'
columns_to_drop = ['year_month', 'date', 'flash_date_str']

# Separate features and target
X = df.drop(columns=[target_column] + [col for col in columns_to_drop if col in df.columns])
y = np.log1p(df[target_column])  # Log transform for better model performance

# Time-based split
train_size = int(0.8 * len(df))
valid_size = int(0.1 * len(df))

X_train = X.iloc[:train_size]
y_train = y.iloc[:train_size]
X_valid = X.iloc[train_size:train_size+valid_size]
y_valid = y.iloc[train_size:train_size+valid_size]
X_test = X.iloc[train_size+valid_size:]
y_test = y.iloc[train_size+valid_size:]

# Hitung jumlah sampel
counts = [len(X_train), len(X_valid), len(X_test)]
labels = ['Training', 'Validation', 'Testing']
percentages = [count / (len(X_train) + len(X_valid) + len(X_test)) * 100 for count in counts]

# Visualisasi
print("Distribusi Jumlah Data:")
print(f"Training\t: {counts[0]} sampel ({percentages[0]:.1f}%)")
print(f"Validation\t: {counts[1]} sampel ({percentages[1]:.1f}%)")
print(f"Testing\t\t: {counts[2]} sampel ({percentages[2]:.1f}%)")
print(f"Total\t\t: {sum(counts)} sampel (100%)")

plt.figure(figsize=(8, 5))
sns.barplot(x=labels, y=counts, hue=labels, palette='Set2', legend=False)

for i, (count, pct) in enumerate(zip(counts, percentages)):
    plt.text(i, count + 5, f'{pct:.1f}%', ha='center', va='bottom', fontsize=12)

plt.title('Distribusi Data: Train / Validation / Test')
plt.ylabel('Jumlah Sampel')
plt.grid(True, axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

import shap
import xgboost as xgb
import lightgbm as lgb
import catboost as cb
from sklearn.ensemble import RandomForestRegressor
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, r2_score

def ensemble_shap_feature_selection(X_train, y_train, X_valid, X_test, top_k=10, verbose=True):
    """
    Feature selection menggunakan ensemble SHAP values dari 4 algoritma:
    XGBoost, LightGBM, CatBoost, dan Random Forest

    Parameters:
    -----------
    X_train, y_train : training data
    X_valid, X_test : validation dan test data
    top_k : jumlah fitur yang akan dipilih
    verbose : print progress atau tidak

    Returns:
    --------
    X_train_selected, X_valid_selected, X_test_selected : data dengan fitur terpilih
    selected_features : list nama fitur yang terpilih
    feature_importance_df : DataFrame dengan importance score
    individual_shap_scores : SHAP scores per model (untuk analisis)
    """

    if verbose:
        print("üöÄ Memulai Ensemble SHAP Feature Selection...")
        print(f"üìä Dataset awal: {X_train.shape[1]} fitur, {X_train.shape[0]} sampel training")
        print("=" * 60)

    # ==========================================
    # DEFINE MODELS
    # ==========================================
    models = {
        'XGBoost': xgb.XGBRegressor(
            random_state=42,
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            verbosity=0
        ),
        'LightGBM': lgb.LGBMRegressor(
            random_state=42,
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            verbosity=-1
        ),
        'CatBoost': cb.CatBoostRegressor(
            random_state=42,
            iterations=100,
            depth=6,
            learning_rate=0.1,
            verbose=False
        ),
        'RandomForest': RandomForestRegressor(
            random_state=42,
            n_estimators=100,
            max_depth=6
        )
    }

    # ==========================================
    # CALCULATE SHAP VALUES FOR EACH MODEL
    # ==========================================
    all_shap_importance = []
    individual_shap_scores = {}
    model_performance = {}

    for name, model in models.items():
        if verbose:
            print(f"üîÑ Processing {name}...")

        try:
            # Train model
            model.fit(X_train, y_train)

            # Make predictions untuk evaluasi
            y_pred = model.predict(X_valid)
            rmse = np.sqrt(mean_squared_error(y_valid, y_pred))
            r2 = r2_score(y_valid, y_pred)
            model_performance[name] = {'RMSE': rmse, 'R2': r2}

            # Calculate SHAP values
            if name == 'CatBoost':
                # CatBoost memerlukan pendekatan khusus
                explainer = shap.Explainer(model)
                shap_values = explainer(X_train.iloc[:500])  # Sample untuk efisiensi
                importance = np.abs(shap_values.values).mean(axis=0)
            else:
                # Untuk XGBoost, LightGBM, Random Forest
                explainer = shap.Explainer(model)
                shap_values = explainer(X_train.iloc[:500])  # Sample untuk efisiensi
                importance = np.abs(shap_values.values).mean(axis=0)

            # Normalisasi importance (0-1)
            importance_normalized = (importance - importance.min()) / (importance.max() - importance.min())

            all_shap_importance.append(importance_normalized)
            individual_shap_scores[name] = importance_normalized

            if verbose:
                print(f"   ‚úÖ {name} - RMSE: {rmse:.4f}, R2: {r2:.4f}")

        except Exception as e:
            if verbose:
                print(f"   ‚ùå Error pada {name}: {str(e)}")
            continue

    if len(all_shap_importance) == 0:
        raise ValueError("Tidak ada model yang berhasil dihitung SHAP values-nya!")

    # ==========================================
    # ENSEMBLE SHAP IMPORTANCE
    # ==========================================
    if verbose:
        print("\nüîÑ Menghitung ensemble importance...")

    # Rata-rata importance dari semua model (equal weight)
    ensemble_importance = np.mean(all_shap_importance, axis=0)

    # Buat DataFrame untuk analisis
    feature_importance_df = pd.DataFrame({
        'Feature': X_train.columns,
        'Ensemble_Importance': ensemble_importance
    })

    # Tambahkan individual scores untuk analisis
    for name, scores in individual_shap_scores.items():
        feature_importance_df[f'{name}_Importance'] = scores

    # Sort berdasarkan ensemble importance
    feature_importance_df = feature_importance_df.sort_values(
        by='Ensemble_Importance',
        ascending=False
    ).reset_index(drop=True)

    # ==========================================
    # SELECT TOP FEATURES
    # ==========================================
    selected_features = feature_importance_df['Feature'].iloc[:top_k].tolist()

    # Apply feature selection
    X_train_selected = X_train[selected_features]
    X_valid_selected = X_valid[selected_features]
    X_test_selected = X_test[selected_features]

    if verbose:
        print(f"\n‚úÖ Feature selection selesai!")
        print(f"üìä Terpilih {len(selected_features)} fitur dari {X_train.shape[1]} fitur awal")
        print(f"üéØ Top 5 fitur: {selected_features[:5]}")
        print(f"üìà Rata-rata ensemble importance: {feature_importance_df['Ensemble_Importance'].mean():.4f}")

    return (X_train_selected, X_valid_selected, X_test_selected,
            selected_features, feature_importance_df, individual_shap_scores)

def visualize_ensemble_shap(feature_importance_df, selected_features, top_k=10):
    """
    Visualisasi hasil ensemble SHAP feature selection
    """
    # Prepare data untuk plotting
    plot_data = feature_importance_df.head(top_k).copy()

    # ==========================================
    # PLOT 1: ENSEMBLE IMPORTANCE
    # ==========================================
    plt.figure(figsize=(12, 8))

    # Main plot
    bars = plt.barh(
        range(len(plot_data)),
        plot_data['Ensemble_Importance'],
        color=plt.cm.viridis(np.linspace(0, 1, len(plot_data)))
    )

    # Customization
    plt.yticks(range(len(plot_data)), plot_data['Feature'])
    plt.xlabel('Ensemble SHAP Importance Score', fontsize=12, fontweight='bold')
    plt.ylabel('Features', fontsize=12, fontweight='bold')
    plt.title(f'Top {top_k} Features - Ensemble SHAP (XGB + LGBM + CatBoost + RF)',
              fontsize=14, fontweight='bold', pad=20)

    # Add value labels
    for i, (bar, val) in enumerate(zip(bars, plot_data['Ensemble_Importance'])):
        plt.text(val + 0.005, bar.get_y() + bar.get_height()/2,
                f'{val:.3f}', va='center', fontweight='bold')

    # Add grid and styling
    plt.grid(True, axis='x', linestyle='--', alpha=0.3)
    plt.tight_layout()
    plt.show()

    # ==========================================
    # PLOT 2: INDIVIDUAL MODEL COMPARISON
    # ==========================================
    model_cols = [col for col in plot_data.columns if col.endswith('_Importance')]

    if len(model_cols) > 0:
        fig, ax = plt.subplots(figsize=(14, 8))

        # Prepare data for heatmap
        heatmap_data = plot_data[['Feature'] + model_cols].set_index('Feature')
        heatmap_data.columns = [col.replace('_Importance', '') for col in heatmap_data.columns]

        # Create heatmap
        sns.heatmap(
            heatmap_data.T,
            annot=True,
            fmt='.3f',
            cmap='viridis',
            cbar_kws={'label': 'SHAP Importance Score'},
            linewidths=0.5
        )

        plt.title(f'Individual Model SHAP Importance - Top {top_k} Features',
                  fontsize=14, fontweight='bold', pad=20)
        plt.xlabel('Features', fontsize=12, fontweight='bold')
        plt.ylabel('Models', fontsize=12, fontweight='bold')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.show()

def analyze_feature_consensus(feature_importance_df, top_k=10):
    """
    Analisis konsensus antar model untuk fitur terpilih
    """
    model_cols = [col for col in feature_importance_df.columns if col.endswith('_Importance')]

    if len(model_cols) == 0:
        print("‚ùå Tidak ada data individual model untuk analisis konsensus")
        return

    print("\n" + "="*60)
    print("üìä ANALISIS KONSENSUS ANTAR MODEL")
    print("="*60)

    top_features = feature_importance_df.head(top_k)

    # Hitung standard deviation untuk setiap fitur (konsensus)
    model_scores = top_features[model_cols].values
    consensus_scores = np.std(model_scores, axis=1)

    # Tambahkan ke DataFrame
    analysis_df = top_features[['Feature', 'Ensemble_Importance']].copy()
    analysis_df['Consensus_Score'] = consensus_scores
    analysis_df['Consensus_Level'] = pd.cut(
        consensus_scores,
        bins=3,
        labels=['High Consensus', 'Medium Consensus', 'Low Consensus']
    )

    print(f"üéØ Top {top_k} features dengan tingkat konsensus:")
    print(analysis_df[['Feature', 'Ensemble_Importance', 'Consensus_Level']].to_string(index=False))

    # Summary
    consensus_summary = analysis_df['Consensus_Level'].value_counts()
    print(f"\nüìà Ringkasan Konsensus:")
    for level, count in consensus_summary.items():
        print(f"   {level}: {count} fitur")

    return analysis_df

# Baseline Model
print("=== BASELINE MODEL ===")
model_baseline = xgb.XGBRegressor(random_state=42, eval_metric=["rmse"])

# Create DMatrix for training and validation
dtrain = xgb.DMatrix(X_train, label=y_train)
dvalid = xgb.DMatrix(X_valid, label=y_valid)

# Set basic model parameters
params_baseline = {
    'objective': 'reg:squarederror',
    'eval_metric': 'rmse',
    'random_state': 42
}

# Prepare eval_set and evaluation results dictionary
evals = [(dtrain, 'train'), (dvalid, 'valid')]
evals_result_baseline = {}

# Train baseline with early stopping
model_baseline_xgb = xgb.train(
    params=params_baseline,
    dtrain=dtrain,
    num_boost_round=1000,
    evals=evals,
    early_stopping_rounds=20,
    evals_result=evals_result_baseline,
    verbose_eval=False
)

# Visualize baseline learning curve
rmse_train_baseline = np.array(evals_result_baseline['train']['rmse'])
rmse_valid_baseline = np.array(evals_result_baseline['valid']['rmse'])
mse_train_baseline = rmse_train_baseline ** 2
mse_valid_baseline = rmse_valid_baseline ** 2

plt.figure(figsize=(10, 6))
plt.plot(mse_train_baseline, label='Train MSE')
plt.plot(mse_valid_baseline, label='Validation MSE')
plt.axvline(x=model_baseline_xgb.best_iteration, color='r', linestyle='--', label='Best Iteration')
plt.title("XGBoost Baseline - MSE Learning Curve")
plt.xlabel("Boosting Rounds")
plt.ylabel("MSE")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# 2. OPTUNA OPTIMIZATION
print("\n2. Starting Optuna Optimization...")

# Time Series Cross Validation
tscv = TimeSeriesSplit(n_splits=3)

def objective_optuna(trial):
    params = {
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.001, 1.0, log=True),
        'n_estimators': trial.suggest_int('n_estimators', 50, 500),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'objective': 'reg:squarederror',
        'random_state': 42,
        'verbosity': 0
    }

    model = xgb.XGBRegressor(**params)

    # Cross-validation score
    scores = []
    for train_idx, val_idx in tscv.split(X_train):
        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]
        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]

        model.fit(X_train_fold, y_train_fold)
        y_pred = model.predict(X_val_fold)
        score = mean_squared_error(y_val_fold, y_pred)
        scores.append(score)

    return np.mean(scores)

# Create Optuna study
study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))
start_optuna = time.time()
study.optimize(objective_optuna, n_trials=200, show_progress_bar=True)
end_optuna = time.time()
print(f"Optuna Optimization completed in {(end_optuna - start_optuna) / 60:.2f} minutes.")

best_params_optuna = study.best_params
print("Best Optuna Parameters:", best_params_optuna)

# Train best Optuna model
model_optuna = xgb.XGBRegressor(**best_params_optuna, random_state=42, verbosity=0)
model_optuna.fit(X_train, y_train)

# === DETAILED LEARNING CURVE FOR OPTUNA ONLY ===
print("\n=== GENERATING LEARNING CURVE FOR OPTUNA OPTIMIZATION ===")

def plot_learning_curve_optuna(model_params, model_name, dtrain, dvalid, y_train, y_valid):
    """Generate learning curves only for Optuna-optimized model (XGBoost)"""

    # Siapkan parameter
    params = model_params.copy()
    params.update({
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse',
        'verbosity': 0,
        'random_state': 42
    })

    # Hapus parameter yang tidak digunakan oleh xgb.train
    params.pop('n_estimators', None)

    evals_result = {}
    evals = [(dtrain, 'train'), (dvalid, 'valid')]

    # Latih model
    model = xgb.train(
        params=params,
        dtrain=dtrain,
        num_boost_round=1000,
        evals=evals,
        early_stopping_rounds=20,
        evals_result=evals_result,
        verbose_eval=False
    )

    # Ambil RMSE dan hitung MSE
    rmse_train = np.array(evals_result['train']['rmse'])
    rmse_valid = np.array(evals_result['valid']['rmse'])
    mse_train = rmse_train ** 2
    mse_valid = rmse_valid ** 2

    # Hitung MAE & R¬≤ per iterasi
    mae_train, mae_valid = [], []
    r2_train, r2_valid = [], []
    for i in range(1, model.best_iteration + 2):
        y_pred_train_i = model.predict(dtrain, iteration_range=(0, i))
        y_pred_valid_i = model.predict(dvalid, iteration_range=(0, i))
        mae_train.append(mean_absolute_error(y_train, y_pred_train_i))
        mae_valid.append(mean_absolute_error(y_valid, y_pred_valid_i))
        r2_train.append(r2_score(y_train, y_pred_train_i))
        r2_valid.append(r2_score(y_valid, y_pred_valid_i))

    # Visualisasi
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle(f'{model_name} - Learning Curves', fontsize=16)

    # MSE
    axes[0, 0].plot(mse_train, label='Train MSE')
    axes[0, 0].plot(mse_valid, label='Validation MSE')
    axes[0, 0].axvline(model.best_iteration, color='r', linestyle='--', label='Best Iteration')
    axes[0, 0].set_title("MSE Curve")
    axes[0, 0].legend()
    axes[0, 0].grid(True)

    # RMSE
    axes[0, 1].plot(rmse_train, label='Train RMSE')
    axes[0, 1].plot(rmse_valid, label='Validation RMSE')
    axes[0, 1].axvline(model.best_iteration, color='r', linestyle='--', label='Best Iteration')
    axes[0, 1].set_title("RMSE Curve")
    axes[0, 1].legend()
    axes[0, 1].grid(True)

    # MAE
    axes[1, 0].plot(mae_train, label='Train MAE')
    axes[1, 0].plot(mae_valid, label='Validation MAE')
    axes[1, 0].axvline(model.best_iteration, color='r', linestyle='--', label='Best Iteration')
    axes[1, 0].set_title("MAE Curve")
    axes[1, 0].legend()
    axes[1, 0].grid(True)

    # R¬≤
    axes[1, 1].plot(r2_train, label='Train R¬≤')
    axes[1, 1].plot(r2_valid, label='Validation R¬≤')
    axes[1, 1].axvline(model.best_iteration, color='r', linestyle='--', label='Best Iteration')
    axes[1, 1].set_title("R¬≤ Curve")
    axes[1, 1].legend()
    axes[1, 1].grid(True)

    plt.tight_layout()
    plt.show()

    return model

# Jalankan untuk model Optuna
model_optuna_detailed = plot_learning_curve_optuna(
    best_params_optuna,
    "XGBoost - Optuna Optimization",
    dtrain, dvalid,
    y_train, y_valid
)

print("\n=== MODEL EVALUATION AND COMPARISON (Baseline vs Optuna) ===")

# Fit baseline model
model_baseline.fit(X_train, y_train)

# Make predictions
y_pred_test_baseline = model_baseline.predict(X_test)
y_pred_test_optuna = model_optuna.predict(X_test)

# Calculate metrics
models_metrics = {
    'Baseline': {
        'MSE': mean_squared_error(y_test, y_pred_test_baseline),
        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_test_baseline)),
        'MAE': mean_absolute_error(y_test, y_pred_test_baseline),
        'R¬≤': r2_score(y_test, y_pred_test_baseline)
    },
    'Optuna': {
        'MSE': mean_squared_error(y_test, y_pred_test_optuna),
        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_test_optuna)),
        'MAE': mean_absolute_error(y_test, y_pred_test_optuna),
        'R¬≤': r2_score(y_test, y_pred_test_optuna)
    }
}

# Comparison DataFrame
metrics_df = pd.DataFrame({
    'Metric': ['MSE', 'RMSE', 'MAE', 'R¬≤'],
    'Baseline': [models_metrics['Baseline']['MSE'], models_metrics['Baseline']['RMSE'],
                 models_metrics['Baseline']['MAE'], models_metrics['Baseline']['R¬≤']],
    'Optuna': [models_metrics['Optuna']['MSE'], models_metrics['Optuna']['RMSE'],
               models_metrics['Optuna']['MAE'], models_metrics['Optuna']['R¬≤']]
}).round(4)

print("\n=== COMPREHENSIVE METRIC COMPARISON ===")
print(metrics_df.to_string(index=False))

# Improvement over baseline
improvement = []
for metric in ['MSE', 'RMSE', 'MAE', 'R¬≤']:
    base = models_metrics['Baseline'][metric]
    opt = models_metrics['Optuna'][metric]
    if metric == 'R¬≤':
        improve = (opt - base) / abs(base) * 100 if base != 0 else float('inf')
    else:
        improve = (base - opt) / base * 100
    improvement.append(improve)

improvement_df = pd.DataFrame({
    'Metric': ['MSE', 'RMSE', 'MAE', 'R¬≤'],
    'Improvement_%': [round(i, 2) for i in improvement]
})

print("\n=== IMPROVEMENT OVER BASELINE (%) ===")
print(improvement_df.to_string(index=False))

# Visualisasi
fig, axes = plt.subplots(2, 2, figsize=(12, 8))
fig.suptitle('Baseline vs Optuna - Performance Comparison', fontsize=16)

for idx, metric in enumerate(['MSE', 'RMSE', 'MAE', 'R¬≤']):
    row, col = divmod(idx, 2)
    values = [models_metrics['Baseline'][metric], models_metrics['Optuna'][metric]]
    bars = axes[row, col].bar(['Baseline', 'Optuna'], values, alpha=0.7)
    best_idx = np.argmin(values) if metric != 'R¬≤' else np.argmax(values)
    bars[best_idx].set_color('green')
    bars[best_idx].set_alpha(0.9)
    axes[row, col].set_title(metric)
    axes[row, col].set_ylabel(metric)
    axes[row, col].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()