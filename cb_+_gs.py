# -*- coding: utf-8 -*-
"""CB + GS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pIx2s3AOCmRAHVJySKtwzxPTL8xUk_qo
"""



!pip install scikit-optimize
!pip install requests
!pip install optuna
!pip install catboost

"""# Import Library"""

# Import libraries
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import requests
import xgboost as xgb
import lightgbm as lgb
import catboost as cb
import optuna
import time
import shap
from lightgbm import LGBMRegressor, early_stopping, log_evaluation, record_evaluation
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import cross_val_score, train_test_split, TimeSeriesSplit, RandomizedSearchCV, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.feature_selection import SelectFromModel
from scipy.stats import uniform, randint
from skopt import BayesSearchCV
from skopt.space import Real, Integer

# Suppress warnings
warnings.filterwarnings('ignore')

# Mount Google Drive and load dataset
from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/My Drive/Dataset/dataset_merged.xlsx'
df = pd.read_excel(file_path)
print(f"Dataset loaded with {df.shape[0]} rows and {df.shape[1]} columns.")

"""# Data Cleaning

"""

# Standarisasi nama kolom
original_columns = df.columns.tolist()
df.columns = df.columns.str.strip().str.replace(" ", "_", regex=False)

# Check missing values
missing_values = df.isnull().sum()
missing_values = missing_values[missing_values > 0]

plt.figure(figsize=(10, 8))
plt.barh(range(len(df.columns)), [1]*len(df.columns))  # Membuat semua batang dengan tinggi 1
plt.yticks(range(len(df.columns)), df.columns)
plt.title('Daftar Fitur dalam Dataset')
plt.xlabel('Jumlah')
plt.ylabel('Fitur (Kolom)')
plt.tight_layout()
plt.show()

# Data Cleaning
df['Size'] = df['Size'].fillna('All_Size').astype(str).str.replace("Ld ", "", case=False).str.strip()
df['Payment_platform_discount'] = pd.to_numeric(df['Payment_platform_discount'], errors='coerce').fillna(0)
df['Handling_Fee'] = pd.to_numeric(df['Handling_Fee'], errors='coerce').fillna(0)

"""# Feature Engineering

Time Extraction
"""

# Simpan kolom asli
original_columns = df.columns.tolist()

# Feature Engineering - Time Features
if 'Created_Time' in df.columns:
    df['Created_Time'] = pd.to_datetime(df['Created_Time'], errors='coerce', dayfirst=True)
    df.dropna(subset=['Created_Time'], inplace=True)

# Extract basic time features
df['year'] = df['Created_Time'].dt.year
df['month'] = df['Created_Time'].dt.month
df['day'] = df['Created_Time'].dt.day
df['hour'] = df['Created_Time'].dt.hour
df['minute'] = df['Created_Time'].dt.minute
df['second'] = df['Created_Time'].dt.second
df['day_of_week'] = df['Created_Time'].dt.dayofweek
df['day_of_year'] = df['Created_Time'].dt.dayofyear
df['quarter'] = df['Created_Time'].dt.quarter
df['week_of_year'] = df['Created_Time'].dt.isocalendar().week

# Cyclical features
df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)

# Calendar/event features
df['is_payday'] = df['day'].isin([25, 26, 27, 28, 29, 30, 31, 1, 2]).astype(int)
df['date'] = df['Created_Time'].dt.date
df['year_month'] = df['Created_Time'].dt.strftime('%Y-%m')
df['year_week'] = df['year'].astype(str) + '-' + df['week_of_year'].astype(str).str.zfill(2)

# Flash sale features
flash_dates = [f"{str(i).zfill(2)}-{str(i).zfill(2)}" for i in range(1, 13)]
df['flash_date_str'] = df['month'].astype(str).str.zfill(2) + '-' + df['day'].astype(str).str.zfill(2)
df['is_flash_sale'] = df['flash_date_str'].isin(flash_dates).astype(int)

# National holidays
years = [2022, 2023, 2024, 2025]
holiday_dates = []

for year in years:
    try:
        url = f"https://date.nager.at/api/v3/PublicHolidays/{year}/ID"
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            data = response.json()
            for holiday in data:
                holiday_dates.append(holiday['date'])
    except:
        continue

if holiday_dates:
    libur_nasional = pd.to_datetime(holiday_dates)
    df['is_holiday'] = df['Created_Time'].dt.date.isin(libur_nasional.date).astype(int)
else:
    df['is_holiday'] = 0

# Additional flags
df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)
df['is_business_hour'] = (((df['hour'] >= 8) & (df['hour'] <= 17)) & (df['day_of_week'] < 5)).astype(int)
df['is_month_start'] = (df['day'] <= 7).astype(int)
df['is_month_end'] = (df['day'] >= 24).astype(int)

# Time period categories
df['time_period'] = pd.cut(df['hour'],
                          bins=[0, 6, 12, 18, 24],
                          labels=['Night', 'Morning', 'Afternoon', 'Evening'],
                          include_lowest=True)

after_time_columns = df.columns.tolist()
added_time_features = [col for col in after_time_columns if col not in original_columns]

# Visualisasi
if added_time_features:
    fig, ax = plt.subplots(figsize=(8, len(added_time_features) * 0.3))
    bars = ax.barh(added_time_features, [1]*len(added_time_features), edgecolor='black')

    for bar in bars:
        ax.text(1.05, bar.get_y() + bar.get_height()/2, '1', va='center', fontsize=10)

    ax.set_title('Fitur Baru dari Feature Engineering Waktu', fontsize=14, pad=10)
    ax.set_xlabel('Fitur Baru')
    ax.set_xlim(0, 1.2)
    ax.set_xticks([])
    ax.invert_yaxis()
    plt.subplots_adjust(left=0.25, right=0.95, top=0.95, bottom=0.05)
    plt.grid(False)
    plt.show()

"""Time - Series"""

# Time Series Features
df = df.sort_values('Created_Time')
df['date'] = df['Created_Time'].dt.date

if 'Product_Name' in df.columns and 'Created_Time' in df.columns:
    # Daily sales aggregation per product
    daily_sales = df.groupby(['Product_Name', 'date'])['Quantity'].sum().reset_index()
    daily_sales = daily_sales.sort_values(['Product_Name', 'date'])

    # Create lag features
    for lag in [1, 7, 14, 30]:
        daily_sales[f'lag_{lag}_days'] = daily_sales.groupby('Product_Name')['Quantity'].shift(lag)
        daily_sales[f'lag_{lag}_days'] = daily_sales.groupby('Product_Name')[f'lag_{lag}_days'].transform(
            lambda x: x.fillna(x.median())
        )

    # Moving Averages & Rolling Sum
    for window in [7, 14, 30]:
        daily_sales[f'moving_avg_{window}'] = daily_sales.groupby('Product_Name')['Quantity'].transform(
            lambda x: x.rolling(window=window, min_periods=max(1, window//2)).mean()
        )
        daily_sales[f'rolling_sum_{window}'] = daily_sales.groupby('Product_Name')['Quantity'].transform(
            lambda x: x.rolling(window=window, min_periods=max(1, window//2)).sum()
        )

    # Merge time series features to main dataframe
    df = pd.merge(
        df,
        daily_sales[['Product_Name', 'date'] +
                    [f'lag_{lag}_days' for lag in [1, 7, 14, 30]] +
                    [f'moving_avg_{window}' for window in [7, 14, 30]] +
                    [f'rolling_sum_{window}' for window in [7, 14, 30]]],
        on=['Product_Name', 'date'],
        how='left'
    )

    # Volatility features
    df['demand_std_7days'] = daily_sales.groupby('Product_Name')['Quantity'].transform(
        lambda x: x.rolling(window=7, min_periods=3).std()
    )
    df['demand_std_7days'] = df.groupby('Product_Name')['demand_std_7days'].transform(
        lambda x: x.fillna(x.median())
    )

    # Sales trends and ratios
    df['sales_trend'] = (df['lag_7_days'] - df['lag_14_days']) / (df['lag_14_days'] + 1e-8) * 100
    df['sales_ratio_to_avg'] = df['Quantity'] / (df['moving_avg_30'] + 1e-8)

    # Clip extreme values
    df['sales_trend'] = df['sales_trend'].clip(-500, 500)
    df['sales_ratio_to_avg'] = df['sales_ratio_to_avg'].clip(0, 10)

# Flash sale features
df['flash_sale_yesterday'] = df.groupby('Product_Name')['is_flash_sale'].shift(1).fillna(0)
df['days_since_last_flash'] = (
    df.groupby('Product_Name')['is_flash_sale']
    .apply(lambda x: (x == 0).cumsum() - (x == 0).cumsum().where(x == 1).ffill().fillna(0))
    .reset_index(level=0, drop=True)
)

# Holiday and payday features
df['holiday_yesterday'] = df['is_holiday'].shift(1).fillna(0)
df['payday_yesterday'] = df['is_payday'].shift(1).fillna(0)

# Additional time features
df['week_of_month'] = pd.to_datetime(df['date']).dt.day.apply(lambda d: (d - 1) // 7 + 1)

# Monthly seasonal index
if 'month' in df.columns and 'Product_Name' in df.columns:
    monthly_avg = df.groupby(['Product_Name', 'month'])['Quantity'].mean().reset_index()
    product_avg = monthly_avg.groupby('Product_Name')['Quantity'].mean().reset_index()

    monthly_avg = pd.merge(monthly_avg, product_avg, on='Product_Name', suffixes=('_month', '_product'))
    monthly_avg['seasonal_index'] = monthly_avg['Quantity_month'] / (monthly_avg['Quantity_product'] + 1e-8)
    monthly_avg['seasonal_index'] = monthly_avg['seasonal_index'].clip(0.1, 5.0)

    df = pd.merge(df, monthly_avg[['Product_Name', 'month', 'seasonal_index']],
                  on=['Product_Name', 'month'], how='left')

# Handle missing values
numerical_cols = df.select_dtypes(include=[np.number]).columns
for col in numerical_cols:
    if col != 'Quantity':
        if df[col].isna().sum() > 0:
            df[col] = df[col].fillna(df[col].median())

# Handle categorical features yang akan menjadi masalah
categorical_cols = df.select_dtypes(include=['category', 'object']).columns
for col in categorical_cols:
    if col in df.columns and col not in ['Created_Time', 'date']:
        if df[col].isna().sum() > 0:
            df[col] = df[col].fillna('Unknown')

# ===== PREPROCESSING UNTUK COMPATIBILITY =====
# Simpan categorical features untuk CatBoost
categorical_features_for_catboost = []

# Convert categorical features menjadi format yang sesuai untuk setiap model
le_dict = {}

# Product_Name encoding
if 'Product_Name' in df.columns:
    le_dict['Product_Name'] = LabelEncoder()
    df['Product_Name_encoded'] = le_dict['Product_Name'].fit_transform(df['Product_Name'].astype(str))
    categorical_features_for_catboost.append('Product_Name')

# Variation encoding
if 'Variation' in df.columns:
    le_dict['Variation'] = LabelEncoder()
    df['Variation_encoded'] = le_dict['Variation'].fit_transform(df['Variation'].astype(str))
    categorical_features_for_catboost.append('Variation')

# Size encoding
if 'Size' in df.columns:
    le_dict['Size'] = LabelEncoder()
    df['Size_encoded'] = le_dict['Size'].fit_transform(df['Size'].astype(str))
    categorical_features_for_catboost.append('Size')

# time_period encoding
if 'time_period' in df.columns:
    le_dict['time_period'] = LabelEncoder()
    df['time_period_encoded'] = le_dict['time_period'].fit_transform(df['time_period'].astype(str))
    categorical_features_for_catboost.append('time_period')

# Sales category
df['sales_category'] = pd.cut(df['Quantity'],
                             bins=[0, 1, 5, 10, 50, np.inf],
                             labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'],
                             include_lowest=True)
le_dict['sales_category'] = LabelEncoder()
df['sales_category_encoded'] = le_dict['sales_category'].fit_transform(df['sales_category'].astype(str))
categorical_features_for_catboost.append('sales_category')

# Interaction features
df['hour_weekend'] = (df['hour'].astype(str) + '_' + df['is_weekend'].astype(str))
le_dict['hour_weekend'] = LabelEncoder()
df['hour_weekend_encoded'] = le_dict['hour_weekend'].fit_transform(df['hour_weekend'])
categorical_features_for_catboost.append('hour_weekend')

df['month_flash'] = (df['month'].astype(str) + '_' + df['is_flash_sale'].astype(str))
le_dict['month_flash'] = LabelEncoder()
df['month_flash_encoded'] = le_dict['month_flash'].fit_transform(df['month_flash'])
categorical_features_for_catboost.append('month_flash')

# Drop columns yang tidak diperlukan
columns_to_drop = ['Created_Time', 'date', 'year_month', 'year_week', 'flash_date_str',
                   'Product_Name', 'Variation', 'Size', 'time_period', 'sales_category',
                   'hour_weekend', 'month_flash']
df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True, errors='ignore')

after_ts_columns = df.columns.tolist()
added_ts_features = [col for col in after_ts_columns if col not in after_time_columns]

# Visualisasi
if added_ts_features:
    fig, ax = plt.subplots(figsize=(8, len(added_ts_features) * 0.3))
    bars = ax.barh(added_ts_features, [1]*len(added_ts_features), edgecolor='black')

    for bar in bars:
        ax.text(1.05, bar.get_y() + bar.get_height()/2, '1', va='center', fontsize=10)

    ax.set_title('Fitur Baru dari Feature Engineering Time Series', fontsize=14, pad=10)
    ax.set_xlabel('Fitur Baru')
    ax.set_xlim(0, 1.2)
    ax.set_xticks([])
    ax.invert_yaxis()
    plt.subplots_adjust(left=0.25, right=0.95, top=0.95, bottom=0.05)
    plt.grid(False)
    plt.show()

"""# Data Split"""

target_column = 'Quantity'

# Separate features and target
X = df.drop(columns=[target_column])
y = df[target_column]

# Time-based split
train_size = int(0.8 * len(df))
valid_size = int(0.1 * len(df))

X_train = X.iloc[:train_size]
y_train = y.iloc[:train_size]
X_valid = X.iloc[train_size:train_size+valid_size]
y_valid = y.iloc[train_size:train_size+valid_size]
X_test = X.iloc[train_size+valid_size:]
y_test = y.iloc[train_size+valid_size:]

# Hitung jumlah sampel
counts = [len(X_train), len(X_valid), len(X_test)]
labels = ['Training', 'Validation', 'Testing']
percentages = [count / (len(X_train) + len(X_valid) + len(X_test)) * 100 for count in counts]

# Visualisasi
print("Distribusi Jumlah Data:")
print(f"Training\t: {counts[0]} sampel ({percentages[0]:.1f}%)")
print(f"Validation\t: {counts[1]} sampel ({percentages[1]:.1f}%)")
print(f"Testing\t\t: {counts[2]} sampel ({percentages[2]:.1f}%)")
print(f"Total\t\t: {sum(counts)} sampel (100%)")

plt.figure(figsize=(8, 5))
sns.barplot(x=labels, y=counts, hue=labels, palette='Set2', legend=False)

for i, (count, pct) in enumerate(zip(counts, percentages)):
    plt.text(i, count + 5, f'{pct:.1f}%', ha='center', va='bottom', fontsize=12)

plt.title('Distribusi Data: Train / Validation / Test')
plt.ylabel('Jumlah Sampel')
plt.grid(True, axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# ===== ENSEMBLE SHAP FEATURE SELECTION =====
top_k = 10
verbose = True

print("🚀 Memulai Ensemble SHAP Feature Selection...")
print(f"📊 Dataset awal: {X_train.shape[1]} fitur, {X_train.shape[0]} sampel training")
print("=" * 60)

# Prepare data untuk masing-masing model
# Untuk XGBoost dan LightGBM: gunakan semua fitur numerik
X_train_numeric = X_train.select_dtypes(include=[np.number])
X_valid_numeric = X_valid.select_dtypes(include=[np.number])

# Untuk CatBoost: gunakan original categorical features
X_train_cat = X_train.copy()
X_valid_cat = X_valid.copy()

# Identifikasi categorical features indices untuk CatBoost
categorical_features_indices = []
for col in categorical_features_for_catboost:
    if col + '_encoded' in X_train_cat.columns:
        categorical_features_indices.append(X_train_cat.columns.get_loc(col + '_encoded'))

print(f"Categorical features indices untuk CatBoost: {categorical_features_indices}")

# Define models dengan parameter yang disesuaikan
models = {
    'XGBoost': {
        'model': xgb.XGBRegressor(
            random_state=42,
            n_estimators=100,
            learning_rate=0.1,
            verbosity=0
        ),
        'X_train': X_train_numeric,
        'X_valid': X_valid_numeric
    },
    'LightGBM': {
        'model': lgb.LGBMRegressor(
            random_state=42,
            n_estimators=100,
            learning_rate=0.1,
            verbosity=-1
        ),
        'X_train': X_train_numeric,
        'X_valid': X_valid_numeric
    },
    'CatBoost': {
        'model': cb.CatBoostRegressor(
            random_state=42,
            iterations=100,
            learning_rate=0.1,
            verbose=False,
            cat_features=categorical_features_indices if categorical_features_indices else None
        ),
        'X_train': X_train_cat,
        'X_valid': X_valid_cat
    }
}

# Calculate SHAP values for each model
all_shap_importance = []
individual_shap_scores = {}
model_performance = {}

for name, model_info in models.items():
    if verbose:
        print(f"🔄 Processing {name}...")

    try:
        model = model_info['model']
        X_train_model = model_info['X_train']
        X_valid_model = model_info['X_valid']

        # Train model
        model.fit(X_train_model, y_train)

        # Evaluasi performa
        y_pred = model.predict(X_valid_model)
        rmse = np.sqrt(mean_squared_error(y_valid, y_pred))
        r2 = r2_score(y_valid, y_pred)
        model_performance[name] = {'RMSE': rmse, 'R2': r2}

        # Hitung SHAP values
        explainer = shap.Explainer(model)
        shap_values = explainer(X_train_model.iloc[:500])
        importance = np.abs(shap_values.values).mean(axis=0)

        # Normalisasi importance
        importance_normalized = (importance - importance.min()) / (importance.max() - importance.min())

        # Untuk alignment dengan feature names yang konsisten
        if name == 'CatBoost':
            # Map CatBoost importance ke feature names yang konsisten
            feature_importance_dict = dict(zip(X_train_model.columns, importance_normalized))
            aligned_importance = []
            for col in X_train_numeric.columns:
                if col in feature_importance_dict:
                    aligned_importance.append(feature_importance_dict[col])
                else:
                    aligned_importance.append(0.0)
            importance_normalized = np.array(aligned_importance)

        all_shap_importance.append(importance_normalized)
        individual_shap_scores[name] = importance_normalized

        if verbose:
            print(f"   ✅ {name} - RMSE: {rmse:.4f}, R2: {r2:.4f}")

    except Exception as e:
        if verbose:
            print(f"   ❌ Error pada {name}: {str(e)}")
        continue

# Check jika ada model yang berhasil
if len(all_shap_importance) == 0:
    raise ValueError("Tidak ada model yang berhasil dihitung SHAP values-nya!")

# Ensemble SHAP importance
print("\n🔄 Menghitung ensemble importance...")
ensemble_importance = np.mean(all_shap_importance, axis=0)

# Buat DataFrame untuk analisis
feature_importance_df = pd.DataFrame({
    'Feature': X_train_numeric.columns,
    'Ensemble_Importance': ensemble_importance
})

# Tambahkan individual scores
for name, scores in individual_shap_scores.items():
    feature_importance_df[f'{name}_Importance'] = scores

# Sort berdasarkan ensemble importance
feature_importance_df = feature_importance_df.sort_values(
    by='Ensemble_Importance',
    ascending=False
).reset_index(drop=True)

# Select top features
selected_features = feature_importance_df['Feature'].iloc[:top_k].tolist()

# Apply feature selection
X_train_selected = X_train_numeric[selected_features]
X_valid_selected = X_valid_numeric[selected_features]
X_test_selected = X_test.select_dtypes(include=[np.number])[selected_features]

print(f"\n✅ Feature selection selesai!")
print(f"📊 Terpilih {len(selected_features)} fitur dari {X_train.shape[1]} fitur awal")
print(f"🎯 Top 5 fitur: {selected_features[:5]}")
print(f"📈 Rata-rata ensemble importance: {feature_importance_df['Ensemble_Importance'].mean():.4f}")

# Visualisasi hasil
print("\n📊 Visualisasi: Ensemble SHAP Importance")
plot_data = feature_importance_df.head(top_k).copy()

plt.figure(figsize=(12, 8))
bars = plt.barh(
    range(len(plot_data)),
    plot_data['Ensemble_Importance'],
    color=plt.cm.viridis(np.linspace(0, 1, len(plot_data)))
)

plt.yticks(range(len(plot_data)), plot_data['Feature'])
plt.xlabel('Ensemble SHAP Importance Score', fontsize=12, fontweight='bold')
plt.ylabel('Features', fontsize=12, fontweight='bold')
plt.title(f'Top {top_k} Features - Ensemble SHAP (XGB + LGBM + CatBoost)',
          fontsize=14, fontweight='bold', pad=20)

# Tambahkan nilai pada bar
for i, (bar, val) in enumerate(zip(bars, plot_data['Ensemble_Importance'])):
    plt.text(val + 0.005, bar.get_y() + bar.get_height()/2,
            f'{val:.3f}', va='center', fontweight='bold')

plt.grid(True, axis='x', linestyle='--', alpha=0.3)
plt.tight_layout()
plt.show()

# Heatmap - SHAP Score per Model
model_cols = [col for col in plot_data.columns if col.endswith('_Importance')]
if len(model_cols) > 0:
    print("\n📊 Visualisasi: SHAP Importance Per Model")
    fig, ax = plt.subplots(figsize=(14, 8))

    heatmap_data = plot_data[['Feature'] + model_cols].set_index('Feature')
    heatmap_data.columns = [col.replace('_Importance', '') for col in heatmap_data.columns]

    sns.heatmap(
        heatmap_data.T,
        annot=True,
        fmt='.3f',
        cmap='viridis',
        cbar_kws={'label': 'SHAP Importance Score'},
        linewidths=0.5
    )

    plt.title(f'Individual Model SHAP Importance - Top {top_k} Features',
              fontsize=14, fontweight='bold', pad=20)
    plt.xlabel('Features', fontsize=12, fontweight='bold')
    plt.ylabel('Models', fontsize=12, fontweight='bold')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

# Analisis konsensus
print("\n" + "="*60)
print("📊 ANALISIS KONSENSUS ANTAR MODEL")
print("="*60)

top_features = feature_importance_df.head(top_k)
model_scores = top_features[model_cols].values
consensus_scores = np.std(model_scores, axis=1)

analysis_df = top_features[['Feature', 'Ensemble_Importance']].copy()
analysis_df['Consensus_Score'] = consensus_scores
analysis_df['Consensus_Level'] = pd.cut(
    consensus_scores,
    bins=3,
    labels=['High Consensus', 'Medium Consensus', 'Low Consensus']
)

print(f"🎯 Top {top_k} features dengan tingkat konsensus:")
print(analysis_df[['Feature', 'Ensemble_Importance', 'Consensus_Level']].to_string(index=False))

consensus_summary = analysis_df['Consensus_Level'].value_counts()
print(f"\n📈 Ringkasan Konsensus:")
for level, count in consensus_summary.items():
    print(f"   {level}: {count} fitur")

print("=== BASELINE MODEL - CATBOOST ===")

from catboost import CatBoostRegressor, Pool

# Inisialisasi model baseline
model_baseline = CatBoostRegressor(random_seed=42, verbose=0, loss_function='RMSE')
model_baseline.fit(X_train, y_train, eval_set=(X_valid, y_valid), use_best_model=True)

# Buat Pool dataset CatBoost
dtrain = Pool(X_train, y_train)
dvalid = Pool(X_valid, y_valid)

# Parameter dasar CatBoost
params_baseline_cat = {
    'loss_function': 'RMSE',
    'verbose': 0,
    'random_seed': 42
}

# Train model dengan struktur sama
model_baseline_cat = CatBoostRegressor(
    iterations=1000,
    **params_baseline_cat
)

model_baseline_cat.fit(
    dtrain,
    eval_set=dvalid,
    use_best_model=True,
    early_stopping_rounds=20,
    verbose=0
)

# Ambil hasil evaluasi setelah training
evals_result_baseline_cat = model_baseline_cat.get_evals_result()

# Ambil nilai RMSE lalu ubah ke MSE
rmse_train_cat = np.array(evals_result_baseline_cat['learn']['RMSE'])
rmse_valid_cat = np.array(evals_result_baseline_cat['validation']['RMSE'])
mse_train_cat = rmse_train_cat ** 2
mse_valid_cat = rmse_valid_cat ** 2

# Visualisasi
plt.figure(figsize=(10, 6))
plt.plot(mse_train_cat, label='Train MSE')
plt.plot(mse_valid_cat, label='Validation MSE')
plt.axvline(x=model_baseline_cat.get_best_iteration(), color='r', linestyle='--', label='Best Iteration')
plt.title("CatBoost Baseline - MSE Learning Curve")
plt.xlabel("Boosting Rounds")
plt.ylabel("MSE")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

print("=== HYPERPARAMETER TUNING METHODS ===")

# Time Series Cross Validation
tscv = TimeSeriesSplit(n_splits=3)

# 1. GRID SEARCH
print("\n1. Starting Grid Search...")
param_grid = {
    'depth': [3, 5, 7, 10],  # setara dengan max_depth
    'learning_rate': [0.001, 0.01, 0.1, 0.3, 0.5],  # dari log-uniform
    'iterations': [50, 100, 200, 300, 500],  # setara dengan n_estimators
    'subsample': [0.6, 0.8, 1.0],  # dari uniform
    'rsm': [0.6, 0.8, 1.0],  # setara dengan colsample_bytree
}

model_catboost_grid = CatBoostRegressor(
    random_state=42,
    verbose=False,
    early_stopping_rounds=20
)

from sklearn.model_selection import GridSearchCV

grid_search = GridSearchCV(
    estimator=model_catboost_grid,
    param_grid=param_grid,
    cv=tscv,
    scoring='neg_mean_squared_error',
    n_jobs=1,
    verbose=3,
    return_train_score=True
)

start_grid = time.time()
grid_search.fit(X_train, y_train)
end_grid = time.time()
print(f"Grid Search completed in {(end_grid - start_grid) / 60:.2f} minutes.")

model_bayes = grid_search.best_estimator_  # hanya ganti nama jadi sama agar kode bawah tetap jalan
best_params_bayes = grid_search.best_params_
print("Best Grid Search Parameters:", best_params_bayes)

print("\n=== GENERATING LEARNING CURVE FOR GRID SEARCH - CATBOOST ===")

def plot_learning_curve_bayes_cat(model_params, model_name, X_train, X_valid, y_train, y_valid):
    """Plot learning curve for Grid Search optimized model (CatBoost)"""

    from catboost import CatBoostRegressor, Pool

    # Salin parameter dan tambahkan param wajib
    params = model_params.copy()
    params.update({
        'loss_function': 'RMSE',
        'verbose': 0,
        'random_seed': 42
    })

    # Hapus 'iterations' agar dimasukkan ke konstruktor
    max_iterations = params.pop('iterations', 1000)

    # Dataset Pool CatBoost
    dtrain = Pool(X_train, y_train)
    dvalid = Pool(X_valid, y_valid)

    # Melatih model penuh untuk mengambil best_iteration
    model_full = CatBoostRegressor(iterations=max_iterations, **params)
    model_full.fit(
        dtrain,
        eval_set=dvalid,
        use_best_model=True,
        early_stopping_rounds=20,
        verbose=0
    )
    best_iteration = model_full.get_best_iteration()

    # Ambil hasil evaluasi
    evals_result = model_full.get_evals_result()
    rmse_train = np.array(evals_result['learn']['RMSE'])[:best_iteration + 1]
    rmse_valid = np.array(evals_result['validation']['RMSE'])[:best_iteration + 1]
    mse_train = rmse_train ** 2
    mse_valid = rmse_valid ** 2

    # Inisialisasi list untuk MAE dan R²
    mae_train, mae_valid = [], []
    r2_train, r2_valid = [], []

    for i in range(1, best_iteration + 2):
        model_i = CatBoostRegressor(iterations=i, **params)
        model_i.fit(X_train, y_train, verbose=0)

        y_pred_train_i = model_i.predict(X_train)
        y_pred_valid_i = model_i.predict(X_valid)

        mae_train.append(mean_absolute_error(y_train, y_pred_train_i))
        mae_valid.append(mean_absolute_error(y_valid, y_pred_valid_i))
        r2_train.append(r2_score(y_train, y_pred_train_i))
        r2_valid.append(r2_score(y_valid, y_pred_valid_i))

    # Visualisasi
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle(f'{model_name} - Learning Curves', fontsize=16)

    axes[0, 0].plot(mse_train, label='Train MSE')
    axes[0, 0].plot(mse_valid, label='Validation MSE')
    axes[0, 0].axvline(best_iteration, color='r', linestyle='--', label='Best Iteration')
    axes[0, 0].set_title("MSE Curve")
    axes[0, 0].legend()
    axes[0, 0].grid(True)

    axes[0, 1].plot(rmse_train, label='Train RMSE')
    axes[0, 1].plot(rmse_valid, label='Validation RMSE')
    axes[0, 1].axvline(best_iteration, color='r', linestyle='--', label='Best Iteration')
    axes[0, 1].set_title("RMSE Curve")
    axes[0, 1].legend()
    axes[0, 1].grid(True)

    axes[1, 0].plot(mae_train, label='Train MAE')
    axes[1, 0].plot(mae_valid, label='Validation MAE')
    axes[1, 0].axvline(best_iteration, color='r', linestyle='--', label='Best Iteration')
    axes[1, 0].set_title("MAE Curve")
    axes[1, 0].legend()
    axes[1, 0].grid(True)

    axes[1, 1].plot(r2_train, label='Train R²')
    axes[1, 1].plot(r2_valid, label='Validation R²')
    axes[1, 1].axvline(best_iteration, color='r', linestyle='--', label='Best Iteration')
    axes[1, 1].set_title("R² Curve")
    axes[1, 1].legend()
    axes[1, 1].grid(True)

    plt.tight_layout()
    plt.show()

    return model_full

# Gunakan best_params dari hasil Grid Search
best_params_bayes = grid_search.best_params_  # Meski berasal dari Grid Search, tetap gunakan variabel ini agar struktur tidak berubah

# Jalankan untuk CatBoost hasil tuning Grid Search
model_bayes_cat_detailed = plot_learning_curve_bayes_cat(
    best_params_bayes,
    "CatBoost - Grid Search",
    X_train, X_valid,
    y_train, y_valid
)

print("\n=== MODEL EVALUATION: BASELINE VS GRID SEARCH (CatBoost) ===")

# Fit baseline CatBoost model
model_baseline.fit(X_train, y_train)

# Predict on test set
y_pred_test_baseline = model_baseline.predict(X_test)
y_pred_test_grid = model_bayes.predict(X_test)  # model_bayes diisi dari grid_search.best_estimator_

# Hitung metrik evaluasi
models_metrics = {
    'Baseline': {
        'MSE': mean_squared_error(y_test, y_pred_test_baseline),
        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_test_baseline)),
        'MAE': mean_absolute_error(y_test, y_pred_test_baseline),
        'R²': r2_score(y_test, y_pred_test_baseline)
    },
    'Grid_Search': {
        'MSE': mean_squared_error(y_test, y_pred_test_grid),
        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_test_grid)),
        'MAE': mean_absolute_error(y_test, y_pred_test_grid),
        'R²': r2_score(y_test, y_pred_test_grid)
    }
}

# Tabel perbandingan metrik
metrics_df = pd.DataFrame({
    'Metric': ['MSE', 'RMSE', 'MAE', 'R²'],
    'Baseline': [models_metrics['Baseline']['MSE'], models_metrics['Baseline']['RMSE'],
                 models_metrics['Baseline']['MAE'], models_metrics['Baseline']['R²']],
    'Grid_Search': [models_metrics['Grid_Search']['MSE'], models_metrics['Grid_Search']['RMSE'],
                    models_metrics['Grid_Search']['MAE'], models_metrics['Grid_Search']['R²']]
}).round(4)

print("\n=== METRIC COMPARISON ===")
print(metrics_df.to_string(index=False))

# Hitung improvement
improvement_data = []
for metric in ['MSE', 'RMSE', 'MAE', 'R²']:
    base = models_metrics['Baseline'][metric]
    opt = models_metrics['Grid_Search'][metric]
    if metric == 'R²':
        improvement = (opt - base) / abs(base) * 100 if base != 0 else float('inf')
    else:
        improvement = (base - opt) / base * 100
    improvement_data.append(improvement)

improvement_df = pd.DataFrame({
    'Metric': ['MSE', 'RMSE', 'MAE', 'R²'],
    'Improvement_%': np.round(improvement_data, 2)
})

print("\n=== IMPROVEMENT OVER BASELINE (%) ===")
print(improvement_df.to_string(index=False))

# Waktu training Grid Search
print(f"\n=== TRAINING TIME (Grid Search - CatBoost) ===")
print(f"Time: {(end_grid - start_grid) / 60:.2f} minutes")

# Cek model terbaik berdasarkan RMSE
if models_metrics['Grid_Search']['RMSE'] < models_metrics['Baseline']['RMSE']:
    best_method = "Grid_Search"
    best_rmse = models_metrics['Grid_Search']['RMSE']
else:
    best_method = "Baseline"
    best_rmse = models_metrics['Baseline']['RMSE']

print(f"\n=== BEST PERFORMING MODEL ===")
print(f"Method: {best_method}")
print(f"Best RMSE: {best_rmse:.4f}")

# Visualisasi
fig, axes = plt.subplots(2, 2, figsize=(12, 8))
fig.suptitle('CatBoost: Baseline vs Grid Search - Performance Comparison', fontsize=16)

for idx, metric in enumerate(['MSE', 'RMSE', 'MAE', 'R²']):
    row, col = divmod(idx, 2)
    values = [models_metrics['Baseline'][metric], models_metrics['Grid_Search'][metric]]
    bars = axes[row, col].bar(['Baseline', 'Grid_Search'], values, alpha=0.7)
    best_idx = np.argmin(values) if metric != 'R²' else np.argmax(values)
    bars[best_idx].set_color('green')
    bars[best_idx].set_alpha(0.9)
    axes[row, col].set_title(metric)
    axes[row, col].set_ylabel(metric)
    axes[row, col].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()